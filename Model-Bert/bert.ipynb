{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirement libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU instead of CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv('D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\website_classification.csv').dropna().reset_index(drop=True)\n",
    "df_main.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the category column's name to Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            website_url  \\\n",
      "0                      https://travelsites.com/expedia/   \n",
      "1                  https://travelsites.com/tripadvisor/   \n",
      "2                 https://www.momondo.in/?ispredir=true   \n",
      "3     https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...   \n",
      "4     https://book.priceline.com/?refid=8431&refclic...   \n",
      "...                                                 ...   \n",
      "1337                           http://www.oldwomen.org/   \n",
      "1338                         http://www.webcamslave.com   \n",
      "1339                        http://www.buyeuroporn.com/   \n",
      "1340  http://www.analdreamhouse.com/30/03/agecheck/i...   \n",
      "1341                     http://www.world-sex-news.com/   \n",
      "\n",
      "                                   cleaned_website_text     Tag  \n",
      "0     official site good hotel accommodation big sav...  Travel  \n",
      "1     expedia hotel book sites like use vacation wor...  Travel  \n",
      "2     tripadvisor hotel book sites like previously d...  Travel  \n",
      "3     cheap flights search compare flights momondo f...  Travel  \n",
      "4     bot create free account create free account si...  Travel  \n",
      "...                                                 ...     ...  \n",
      "1337  electroshops home theater decor interiors seat...   Adult  \n",
      "1338  clean ridge soap company clean ridge soap comp...   Adult  \n",
      "1339  home page pet crafts exquisitely piece handcut...   Adult  \n",
      "1340  home theater marketplace home theater seating ...   Adult  \n",
      "1341  thrive market healthy living easy buy healthy ...   Adult  \n",
      "\n",
      "[1342 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_main.rename(columns={'Category': 'Tag'}, inplace=True)\n",
    "df_main.columns\n",
    "print(df_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the list of Tags in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Education',\n",
       " 'Law and Government',\n",
       " 'Streaming Services',\n",
       " 'Sports',\n",
       " 'Health and Fitness',\n",
       " 'Games',\n",
       " 'E-Commerce',\n",
       " 'Computers and Technology',\n",
       " 'Business/Corporate',\n",
       " 'News',\n",
       " 'Social Networking and Messaging',\n",
       " 'Photography',\n",
       " 'Travel',\n",
       " 'Forums',\n",
       " 'Food',\n",
       " 'Adult']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tag = list(set(df_main['Tag']))\n",
    "Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the labels(tags) into number(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {Tag[i]: i for i in range(len(Tag))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: Tag[i] for i in range(len(Tag))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[\"Tags\"] = [label2id[df_main[\"Tag\"][i]] for i in range(len(df_main))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df_main[\"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data for train,test,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_main[\"cleaned_website_text\"],df_main['Tags'], test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a python dictionary into a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_dict({\"text\": X_train, \"labels\": y_train}),\n",
    "        \"test\": Dataset.from_dict({\"text\": X_test, \"labels\": y_test}),\n",
    "        \"val\": Dataset.from_dict({\"text\": X_val, \"labels\": y_val})\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 939\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 282\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 121\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model id to load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "model_id = \"bert-base-uncased\"\n",
    "save_dataset_path = \"lm_dataset\"\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Tokenize helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize dataset\n",
    "# dataset = dataset.rename_column(\"label\", \"labels\") # to match Trainer\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "# tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "# save dataset to disk\n",
    "tokenized_dataset[\"train\"].save_to_disk(os.path.join(save_dataset_path,\"train\"))\n",
    "tokenized_dataset[\"test\"].save_to_disk(os.path.join(save_dataset_path,\"test\"))\n",
    "tokenized_dataset[\"val\"].save_to_disk(os.path.join(save_dataset_path,\"val\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These components are essential for setting up and training machine learning models, particularly for sequence classification tasks in natural language processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# from optimum.neuron import NeuronTrainer as Trainer\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A basic structure for training a machine learning model for sequence classification using the Hugging Face transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    ...\n",
    "def training_function(args):\n",
    "    # load dataset from disk and tokenizer\n",
    "    train_dataset = load_from_disk(os.path.join(args.dataset_path, \"train\"))\n",
    "    # Download the model from huggingface.co/models\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    "    )\n",
    "    training_args = TrainingArguments(\n",
    "        ...\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arina\\AppData\\Local\\Temp\\ipykernel_28020\\2700626002.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "c:\\Users\\Arina\\anaconda3\\envs\\bert\\Lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(val_pred):\n",
    "    logits, labels = val_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detailed and corrected setup for training a sequence classification model using the Hugging Face transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_from_disk(os.path.join(save_dataset_path, \"train\"))\n",
    "val_dataset = load_from_disk(os.path.join(save_dataset_path, \"val\"))\n",
    "\n",
    "# Download the model from huggingface.co/models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=16\n",
    ")\n",
    "\n",
    "# Corrected training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_dir3\",  # Specify the directory where the output files will be saved\n",
    "    evaluation_strategy=\"steps\",  # Corrected from 'valuation_strategy'\n",
    "    eval_steps=500,  # Corrected from 'val_steps' to 'eval_steps'\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,  # Corrected from 'per_device_val_batch_size' to 'per_device_eval_batch_size'\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    # Optional arguments (uncomment and use as needed)\n",
    "    # seed=0,\n",
    "    # learning_rate=0.000001,\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # Corrected from 'val_dataset' to 'eval_dataset'\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\First-version-model\"\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the tokenizer of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\second-version-model\\\\tokenizer_config.json',\n",
       " 'D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\second-version-model\\\\special_tokens_map.json',\n",
       " 'D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\second-version-model\\\\vocab.txt',\n",
       " 'D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\second-version-model\\\\added_tokens.json',\n",
       " 'D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\second-version-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_save_path = \"D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\second-version-model\"\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to test the model with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.tokenizer(self.texts[idx], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "#         item = {key: val.squeeze() for key, val in item.items()}  # Squeeze the batch dimension\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "# # Assuming you have a CSV file with your dataset\n",
    "# # df = pd.read_csv('D:\\\\Rayka_Company\\\\Azar\\\\Work\\\\ML-NFstream\\\\Model-Bert\\\\website_classification.csv')\n",
    "# test_df = df_main.sample(frac=0.2)  # Let's say we use 20% of the data for testing\n",
    "\n",
    "# test_dataset = CustomDataset(\n",
    "#     texts=test_df['cleaned_website_text'].tolist(),  # Replace 'text_column_name' with the actual column name\n",
    "#     labels=test_df['Tag'].tolist(),  # Replace 'label_column_name' with the actual column name\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, data_loader):\n",
    "#     model.eval()\n",
    "#     predictions, labels = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in data_loader:\n",
    "#             input_ids = batch['input_ids']\n",
    "#             attention_mask = batch['attention_mask']\n",
    "#             labels.extend(batch['labels'].numpy())\n",
    "            \n",
    "#             outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#             logits = outputs.logits\n",
    "#             preds = torch.argmax(logits, dim=1).numpy()\n",
    "#             predictions.extend(preds)\n",
    "\n",
    "#     accuracy = accuracy_score(labels, predictions)\n",
    "#     return accuracy\n",
    "\n",
    "# accuracy = evaluate(model, test_loader)\n",
    "# print(f\"Model accuracy on the test set: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
